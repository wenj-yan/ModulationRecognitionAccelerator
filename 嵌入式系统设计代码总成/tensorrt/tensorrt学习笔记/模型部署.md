# 部署体验

有了上节课的tensorRT基础，我们这里以 TensorRT 所提供的 YOLOv3 范例来做范例

在安装 **Jetpack 4.6 版本**的 [Jetson](http://nvidia.zhidx.com/tag/Jetson.html) Nano 设备上进行体验，请进入到 TesnorRT 的 YOLOv3 范例中：

```shell
cd /usr/src/tensorrt/samples/python/yolov3_onnx
```

根据项目的 **README.md** 指示，我们需要先为工作环境添加依赖库，不过由于部分库的版本关系，请先将 **requirements.txt** 的第 1、3 行进行以下的修改：

```shell
numpy==1.19.4protobuf>=3.11.3onnx==1.10.1Pillow; python_version<"3.6"Pillow==8.1.2; python_version>="3.6"pycuda<2021.1
```

然后执行以下指令进行安装:

```shell
python3 -m pip install -r requirements.txt
```

接下来需要先下载 **download.yml** 里面的三个文件:

```shell
wget https://pjreddie.com/media/files/yolov3.weights wget https://raw.githubusercontent.com/pjreddie/darknet/f86901f6177dfc6116360a13cc06ab680e0c86b0/cfg/yolov3.cfg wget https://github.com/pjreddie/darknet/raw/f86901f6177dfc6116360a13cc06ab680e0c86b0/data/dog.jpg 
```

然后就能执行以下指令，将 **yolov3.weights** 转成 **yolov3.onnx**：

```shell
./yolov3_to_onnx.py -d /usr/src/tensorrt
```

这个执行并不复杂，**<u>是因为 TensorRT 已经提供 yolov3_to_onnx.py 的 Python 代码</u>**，但如果将代码打开之后，就能感受到这 750+ 行代码要处理的内容是相当复杂，必须对 YOLOv3 的结构与算法有足够了解，包括解析 yolov3.cfg 的 788 行配置。想象一下，如果这个代码需要自行开发的话，这个难度有多高！

接下去再用下面指令，将 **yolov3.onnx** 转成 **yolov3.trt** 加速引擎：

```shell
./onnx_to_tensorrt.py -d /usr/src/tensorrt
```

以上是从一般[神经网络](http://nvidia.zhidx.com/tag/神经网络.html)模型转成 TensorRT 加速引擎的标准步骤，这需要对所使用的神经网络的结构层、数学公式、参数细节等等都有相当足够的了解，才有能力将模型先转换成 ONNX 文件，这是技术门槛比较高的环节。

